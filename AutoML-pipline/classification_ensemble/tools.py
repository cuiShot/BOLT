import os
import sys
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__),   # å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
                 "..", "..")                  # å‘ä¸Šè·³ä¸‰çº§
)
sys.path.append(project_root)

import re
import numpy as np
import pandas as pd
import pickle
import torch
from openai import OpenAI
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.base import BaseEstimator, ClassifierMixin
from typing import List
import copy



import os
import pickle
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split


def load_origin_data(dataset_name, seed=0):
    # éœ€è¦èµ° .pkl çš„æ—§æ•°æ®é›†å…³é”®è¯ï¼ˆå­ä¸²åŒ¹é…ï¼‰
    old_keys = ('credit','cd1','cc1','ld1','cc2','cd2','cf1','balance-scale')
    name_l = dataset_name.lower()
    is_old = any(k in name_l for k in old_keys)

    if is_old:
        loc = f"{project_root}/tests/data/{dataset_name}.pkl"
        with open(loc, 'rb') as f:
            ds = pickle.load(f)

        # credit ç³»åˆ—éœ€è¦å…ˆ splitï¼Œå…¶å®ƒæ—§æ•°æ®é›†ç›´æ¥ ds[1]/ds[2]
        if 'credit' in name_l:
            ds, df_train, df_test = get_data_split(ds, seed=seed)
        else:
            df_train, df_test = ds[1], ds[2]

        target_column_name = ds[4][-1]
        dataset_description = ds[-1]
        return df_train, df_test, target_column_name, dataset_description

    # å¦åˆ™è¯»å–æ–°çš„ CSV æ•°æ®é›†
    base_loc = f"{project_root}/new_dataSet/binaryclass/"
    return load_new_dataset(dataset_name, base_loc=base_loc, seed=seed, test_size=0.2)


from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, LabelEncoder

def _normalize_target_to_int(s: pd.Series) -> pd.Series:
    """æŠŠç›®æ ‡åˆ—ç»Ÿä¸€è½¬æˆæ•´å‹æ ‡ç­¾ï¼šäºŒåˆ†ç±»'yes/no'ç­‰â†’0/1ï¼›å…¶å®ƒæƒ…å†µâ†’LabelEncoderçš„0..K-1ã€‚"""
    # ä¸¢æ‰é¦–å°¾ç©ºç™½ï¼Œç»Ÿä¸€å°å†™å­—ç¬¦ä¸²è§†å›¾ï¼ˆä¸æ”¹å˜åŸå€¼ï¼‰
    s_str = s.astype("string").str.strip().str.lower()

    # å¸¸è§äºŒåˆ†ç±»æ–‡æ¡ˆæ˜ å°„
    map01 = {
        "yes": 1, "y": 1, "true": 1, "t": 1, "positive": 1, "pos": 1, "1": 1,
        "no": 0,  "n": 0, "false": 0,"f": 0, "negative": 0, "neg": 0, "0": 0
    }

    # çº¯æ•°å€¼/å¸ƒå°”ç›´æ¥è½¬
    if pd.api.types.is_bool_dtype(s):
        return s.astype(int)
    if pd.api.types.is_numeric_dtype(s):
        # è‹¥æ˜¯0/1æµ®ç‚¹ï¼Œç›´æ¥è½¬ï¼›å¦åˆ™å››èˆäº”å…¥ï¼ˆæŒ‰éœ€å¯æ”¹ï¼‰
        uniq = set(pd.unique(s.dropna()))
        if uniq <= {0, 1, 0.0, 1.0}:
            return s.astype(int)
        return s.round().astype(int)

    # çº¯äºŒåˆ†ç±»æ–‡æ¡ˆæ—¶ï¼ŒæŒ‰æ˜ å°„è½¬
    non_na = s_str.dropna()
    if not non_na.empty and non_na.isin(map01).all():
        return s_str.map(map01).astype(int)

    # å…¶ä»–æƒ…å†µï¼ˆå«å¤šåˆ†ç±»/æ··åˆæ–‡æ¡ˆï¼‰ï¼šLabelEncoder åˆ° 0..K-1
    le = LabelEncoder()
    enc = le.fit_transform(s_str.fillna("__missing__"))
    return pd.Series(enc, index=s.index, dtype=int)

def get_data_split(ds, seed):
    def get_df(X, y):
        df = pd.DataFrame(
            data=np.concatenate([X, np.expand_dims(y, -1)], -1), columns=ds[4]
        )
        cat_features = ds[3]
        for c in cat_features:
            if len(np.unique(df.iloc[:, c])) > 50:
                cat_features.remove(c)
                continue
            df[df.columns[c]] = df[df.columns[c]].astype("int32")
        return df.infer_objects()

    ds = copy.deepcopy(ds)

    X = ds[1].numpy() if type(ds[1]) == torch.Tensor else ds[1]
    y = ds[2].numpy() if type(ds[2]) == torch.Tensor else ds[2]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed
    )

    df_train = get_df(X_train, y_train)
    df_test = get_df(X_test, y_test)
    df_train.iloc[:, -1] = df_train.iloc[:, -1].astype("category")
    df_test.iloc[:, -1] = df_test.iloc[:, -1].astype("category")

    return ds, df_train, df_test


def load_new_dataset(dataset_name: str,
                     base_loc: str = f"{project_root}/AutoML_pipline/new_dataSet/binaryclass/",
                     seed: int = 42,
                     test_size: float = 0.2):
    """
    è¯»å– {base_loc}/{dataset_name}.csvï¼Œåšæœ€å°é¢„å¤„ç†å¹¶åˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•ã€‚
    è¿”å›: df_train, df_test, target_column_name, dataset_description
    - df_* å«ç›®æ ‡åˆ—ï¼ˆæœ€åä¸€åˆ—ï¼‰ï¼Œä¸æ‹† X/y
    - é¢„å¤„ç†ä»…ä½œç”¨äºç‰¹å¾åˆ—ï¼Œç›®æ ‡åˆ—è½¬ä¸º int å¹¶ä¿ç•™åœ¨æœ€å
    """
    # 1) è¯»æ•°æ®
    loc = os.path.join(base_loc, dataset_name + ".csv")
    df = pd.read_csv(loc).convert_dtypes()

    # 2) ç›®æ ‡åˆ—å = æœ€åä¸€åˆ—ï¼›å»æ‰æ²¡æœ‰æ ‡ç­¾çš„è¡Œ
    target_column_name = df.columns[-1]
    df = df[~df[target_column_name].isna()].copy()

    # 3) å…ˆæŠŠç›®æ ‡åˆ—è§„èŒƒä¸ºæ•´å‹ï¼ˆé˜²æ­¢åç»­ to_pd é‡Œ astype(int) æŠ¥é”™ï¼‰
    df[target_column_name] = _normalize_target_to_int(df[target_column_name])

    # 4) å…ˆåˆ‡åˆ†æ•´è¡¨ï¼ˆä¸æ³„æ¼ï¼‰ï¼›è¿™é‡Œç”¨æ™®é€šéšæœºåˆ‡åˆ†ï¼Œè‹¥éœ€åˆ†å±‚å¯åŠ è‡ªå®šä¹‰åˆ†å±‚
    df_train_raw, df_test_raw = train_test_split(df, test_size=test_size, random_state=seed)

    # 5) ä»…å¯¹ç‰¹å¾åˆ—åšé¢„å¤„ç†ï¼ˆæ•°å€¼å¡«0ï¼›ç±»åˆ«åºæ•°ç¼–ç ï¼ŒæœªçŸ¥â†’-1ï¼‰
    feature_cols = [c for c in df.columns if c != target_column_name]
    X_train = df_train_raw[feature_cols]
    X_test  = df_test_raw[feature_cols]

    num_cols = X_train.select_dtypes(include=["number", "boolean"]).columns.tolist()
    cat_cols = [c for c in feature_cols if c not in num_cols]

    num_pipe = Pipeline([
        ("impute", SimpleImputer(strategy="constant", fill_value=0)),
    ])
    cat_pipe = Pipeline([
        ("impute", SimpleImputer(strategy="constant", fill_value="__MISSING__")),
        ("encode", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)),
    ])

    preproc = ColumnTransformer(
        transformers=[
            ("num", num_pipe, num_cols),
            ("cat", cat_pipe, cat_cols),
        ],
        remainder="drop",
        verbose_feature_names_out=False,
    )

    X_train_enc = preproc.fit_transform(X_train)
    X_test_enc  = preproc.transform(X_test)

    feature_order = num_cols + cat_cols
    df_train_feat = pd.DataFrame(X_train_enc, columns=feature_order, index=X_train.index)
    df_test_feat  = pd.DataFrame(X_test_enc,  columns=feature_order, index=X_test.index)

    # 6) ç›®æ ‡åˆ—è¿½åŠ å›æœ€åï¼ˆå·²æ˜¯ intï¼‰
    df_train = pd.concat(
        [df_train_feat.reset_index(drop=True),
         df_train_raw[target_column_name].reset_index(drop=True)],
        axis=1
    )
    df_test = pd.concat(
        [df_test_feat.reset_index(drop=True),
         df_test_raw[target_column_name].reset_index(drop=True)],
        axis=1
    )

    # 7) æ•°æ®é›†æè¿°
    dataset_description = read_txt_file(os.path.join(base_loc, f"{dataset_name}-description.txt"))

    return df_train, df_test, target_column_name, dataset_description

# è¯»å–å…¶ä»–æ•°æ®é›†
def load_origin_data_1(loc):
    # è¯»å–æ•°æ®é›†
    with open(loc, 'rb') as f:
        ds = pickle.load(f)

    df_train = ds[1]
    df_test = ds[2]
    target_column_name = ds[4][-1]
    dataset_description = ds[-1]

    return df_train, df_test, target_column_name, dataset_description
# è¯»å–creditæ•°æ®é›†
def load_credit_origin_data(loc):
    # è¯»å–æ•°æ®é›†
    with open(loc, 'rb') as f:
        ds = pickle.load(f)
    ds, df_train, df_test = get_data_split(ds, seed=0)
    target_column_name = ds[4][-1]
    dataset_description = ds[-1]

    return df_train, df_test, target_column_name, dataset_description
# åŠŸèƒ½ï¼šç”ŸæˆåŸºç¡€æ¨¡å‹ éšæœºæ£®æ—
def base_model(seed):
    rforest = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight='balanced')
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  # å¯é‡å¤çš„éšæœºæ•°æ®åˆ’åˆ†
    param_grid = {
        "min_samples_leaf": [0.001, 0.01, 0.05],  # è°ƒæ•´èŒƒå›´
        "max_depth": [5, 10, None]  # æ–°å¢æ·±åº¦æ§åˆ¶
    }
    gsmodel = GridSearchCV(rforest, param_grid, cv=cv, scoring='f1')

    return gsmodel
# åŠŸèƒ½ï¼šæ‰§è¡Œä»£ç å¹¶æ•è·é”™è¯¯
def code_exec(code):
    try:
        # å°è¯•ç¼–è¯‘æ£€æŸ¥ï¼ˆcompile æˆ AST å†æ‰§è¡Œï¼‰
        compiled_code = compile(code, "<string>", "exec")
        exec(compiled_code, globals())
        return None
    except Exception as e:
        print("Code could not be executed:", e)
        return str(e)
# åŠŸèƒ½ï¼šæ¸…ç† LLM ç”Ÿæˆçš„ä»£ç 
def clean_llm_code(code: str) -> str:
    import re
    # å»é™¤ ``` å¼€å¤´çš„ä»£ç å—æ ‡è®°å’Œæœ«å°¾é™„åŠ å†…å®¹
    code = re.sub(r"^```python\s*", "", code.strip(), flags=re.IGNORECASE)
    code = re.sub(r"```$", "", code.strip())

    # æ¸…é™¤ <end> å’Œéä»£ç æ–‡å­—ï¼ˆå¯èƒ½æ¥è‡ª LLMï¼‰
    code = re.sub(r"<end>", "", code)

    # ç§»é™¤ LLM è¾“å‡ºä¸­çš„è§£é‡Šæ®µæˆ–æ–‡æœ¬å¼€å¤´é”™è¯¯æç¤º
    lines = code.strip().splitlines()
    cleaned_lines = []
    for line in lines:
        if line.strip().startswith("class myclassifier") or line.strip().startswith("import") or line.strip().startswith(
                "from"):
            cleaned_lines.append(line)
        elif cleaned_lines:  # å¦‚æœå·²å¼€å§‹è®°å½•ä»£ç å—ï¼Œç»§ç»­æ·»åŠ åç»­ä»£ç 
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines)
# # å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
# def train_test_split(df, test_size=0.20, random_state=None):
#     """    
#     å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
#     :param df: æ•°æ®é›†
#     :param test_size: éªŒè¯é›†å æ¯”
#     :param random_state: éšæœºç§å­
#     :return: è®­ç»ƒé›†å’ŒéªŒè¯é›†
#     """
#     if random_state is not None:
#         np.random.seed(random_state)
    
#     # æ‰“ä¹±æ•°æ®é›†
#     shuffled_indices = np.random.permutation(len(df))
#     test_set_size = int(len(df) * test_size)
    
#     test_indices = shuffled_indices[:test_set_size]
#     train_indices = shuffled_indices[test_set_size:]
    
#     df_train = df.iloc[train_indices].reset_index(drop=True)
#     df_val = df.iloc[test_indices].reset_index(drop=True)
    
#     return df_train, df_val
# è¯»å–txtæ–‡ä»¶çš„å…¨éƒ¨å†…å®¹
def read_txt_file(file_path):
    """
    è¯»å–txtæ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œä¿ç•™æ ¼å¼ï¼Œè¿”å›å­—ç¬¦ä¸²ã€‚
    
    å‚æ•°ï¼š
        file_path (str): txt æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
    
    è¿”å›ï¼š
        str: æ–‡ä»¶å†…å®¹ï¼ˆä¿ç•™æ¢è¡Œã€ç©ºæ ¼ç­‰æ ¼å¼ï¼‰ï¼Œè¯»å–å¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        print(f"[é”™è¯¯] æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
    except UnicodeDecodeError:
        print(f"[é”™è¯¯] æ–‡ä»¶ç¼–ç æ— æ³•è¯†åˆ«ï¼ˆå°è¯•ä½¿ç”¨ UTF-8 ç¼–ç å¤±è´¥ï¼‰: {file_path}")
    except PermissionError:
        print(f"[é”™è¯¯] æ²¡æœ‰æƒé™è¯»å–è¯¥æ–‡ä»¶: {file_path}")
    except Exception as e:
        print(f"[é”™è¯¯] è¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return None
# ç”Ÿæˆ single-shot
def generate_single_shot(dataset_description_prompt,
                         model_code_prompt,
                         model_performance_differences_prompt,
                         best_weights,
                         best_auc,
                         output_txt_path="single_shot_prompt.txt"):
    """
    æ„é€  single-shot æç¤ºè¯å¹¶å†™å…¥åˆ° txt æ–‡ä»¶ã€‚

    :param dataset_description_prompt: æ•°æ®é›†æè¿°éƒ¨åˆ†ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param model_code_prompt: æ¨¡å‹ä»£ç ä¸æ€§èƒ½éƒ¨åˆ†ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param model_performance_differences_prompt: æ¨¡å‹å·®å¼‚çŸ©é˜µéƒ¨åˆ†ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param best_weights: æœ€ä¼˜æƒé‡åˆ—è¡¨æˆ–å…ƒç»„
    :param best_auc: æœ€ä¼˜ AUC å€¼
    :param output_txt_path: è¾“å‡º txt æ–‡ä»¶è·¯å¾„
    :return: å†™å…¥çš„å®Œæ•´ single-shot prompt å­—ç¬¦ä¸²
    """
    # æ ¼å¼åŒ–æƒé‡å‘é‡ä¸ºè¾“å‡ºå­—ç¬¦ä¸²
    weight_lines = []
    for i, w in enumerate(best_weights):
        weight_lines.append(f"æ¨¡å‹{i+1}ï¼šæƒé‡ = {w:.4f}")
    weight_text = "\n".join(weight_lines)

    # æ„é€  single-shot å†…å®¹
    shot_prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æ ¹æ®å¤šä¸ªæ¨¡å‹çš„è¡¨ç°ä¸äº’è¡¥æ€§ä¿¡æ¯åˆ†é…åˆç†çš„é›†æˆæƒé‡ã€‚

    ä»»åŠ¡æè¿°ï¼š
    ä½ å°†çœ‹åˆ°è‹¥å¹²åˆ†ç±»æ¨¡å‹åŠå…¶åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ç»™å‡ºæ¨¡å‹ä¹‹é—´çš„é¢„æµ‹å·®å¼‚çŸ©é˜µã€‚ä½ çš„ç›®æ ‡æ˜¯ä¸ºè¿™äº›æ¨¡å‹åˆ†é…åˆç†çš„é›†æˆæƒé‡ï¼ˆéè´Ÿï¼Œä¸”æ€»å’Œä¸º1ï¼‰ï¼Œä»¥åœ¨éªŒè¯é›†ä¸Šè·å¾—æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

    {dataset_description_prompt.strip()}
    {model_code_prompt.strip()}
    {model_performance_differences_prompt.strip()}

    ã€æœ€ä½³æƒé‡å‘é‡å»ºè®®ã€‘
    {weight_text}

    æœ€ä¼˜éªŒè¯é›† AUCï¼š{best_auc:.4f}

    åŠ æƒç†ç”±ç¤ºèŒƒï¼š
    - æ¨¡å‹1å…·æœ‰æœ€é«˜çš„ AUCï¼Œå› æ­¤ç»™äºˆæœ€é«˜æƒé‡ï¼›
    - æ¨¡å‹2æ€§èƒ½ç•¥ä½ï¼Œä½†ç¨³å®šæ€§è¾ƒå¥½ï¼›
    - æ¨¡å‹3è™½ç„¶æ€§èƒ½åå¼±ï¼Œä½†åœ¨é¢„æµ‹ä¸Šä¸æ¨¡å‹1å’Œ2äº’è¡¥æ€§å¼ºï¼Œå› æ­¤ä¿ç•™ä¸€å®šæƒé‡ï¼›
    - æ•´ä½“ç­–ç•¥å…¼é¡¾äº†ä¸ªä½“æ€§èƒ½ä¸æ¨¡å‹é—´å·®å¼‚ï¼Œæå‡é›†æˆæ¨¡å‹çš„é²æ£’æ€§ã€‚
    """

    # å†™å…¥æ–‡ä»¶
    with open(output_txt_path, 'w', encoding='utf-8') as f:
        f.write(shot_prompt)

    print(f"âœ… single-shot æç¤ºè¯å·²å†™å…¥åˆ°ï¼š{output_txt_path}")
    return shot_prompt

# def get_dataset_description(ds_name, dataset_description):
#     """
#     è·å–æ•°æ®é›†æè¿°çš„æç¤ºè¯
#     :param ds_name: æ•°æ®é›†åç§°
#     :param dataset_description: æ•°æ®é›†æè¿°
#     :return: æ•°æ®é›†æè¿°çš„æç¤ºè¯
#     """
#     loc = "/home/usr01/cuicui/autoML-ensemble/data/" + ds_name + ".json"
#     # è¯»å–æ•°æ®é›†ç‰¹å¾æè¿°jsonæ–‡ä»¶å…¨éƒ¨å†…å®¹
#     if os.path.exists(loc):
#         with open(loc, 'r', encoding='utf-8') as f:
#             feature_description = f.read()
#     else:
#         print(f"æ•°æ®é›†æè¿°æ–‡ä»¶ {loc} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æè¿°ã€‚")

#     return f"""
#     æ•°æ®é›†åç§°ï¼š{ds_name}
#     æ•°æ®é›†æè¿°ï¼š{dataset_description}
#     æ•°æ®é›†ç‰¹å¾æè¿°ï¼š{feature_description}
#     """

# åŠŸèƒ½ï¼šè·å–æ¨¡å‹ä»£ç ä¸æ¨¡å‹è¡¨ç°æç¤ºè¯
def get_model_code_prompt(model_code_list=None,
                          val_acc_list=None,
                          val_auc_list=None,
                          val_f1_list=None,
                          val_pre_list=None,
                          val_rec_list=None,
                          use_code=False):
    """
    è·å–æ¨¡å‹ä»£ç ä¸æ¨¡å‹è¡¨ç°æç¤ºè¯
    :param model_code_list: æ¨¡å‹ä»£ç åˆ—è¡¨
    :param val_acc_list: éªŒè¯é›†å‡†ç¡®ç‡åˆ—è¡¨
    """
    model_code_prompt = "Model list:\n"
    for i, code in enumerate(model_code_list):
        model_code_prompt += f"Model {i + 1}:\n"
        if use_code:
            model_code_prompt += f"code: {code}\n"
        # æ”¶é›†è¯¥æ¨¡å‹çš„æ‰€æœ‰æœ‰æ•ˆæŒ‡æ ‡
        metrics = []
        if val_acc_list is not None:
            metrics.append(f"Accuracy={val_acc_list[i]:.2f},")
        if val_auc_list is not None:
            metrics.append(f"AUC={val_auc_list[i]:.2f},")
        if val_pre_list is not None:
            metrics.append(f"Precision={val_pre_list[i]:.2f},")
        if val_rec_list is not None:
            metrics.append(f"Recall={val_rec_list[i]:.2f},")
        if val_f1_list is not None:
            metrics.append(f"F1={val_f1_list[i]:.2f}")
        
        # ä»…å½“æœ‰æŒ‡æ ‡æ—¶æ‰æ·»åŠ è¡¨ç°ä¿¡æ¯
        if metrics:
            model_code_prompt += f"- Validation set performance: {' '.join(metrics)}\n"
    
    return model_code_prompt

# åŠŸèƒ½ï¼šè·å–æ¨¡å‹è¡¨ç°å·®å¼‚çŸ©é˜µ,æ¨¡å‹åœ¨ç‹¬ç«‹éªŒè¯é›†ä¸Šçš„é¢„æµ‹å·®å¼‚çŸ©é˜µ
def get_model_pre_differences(best_fitted_model_instance_list,val_x,val_y):
    """
    è®¡ç®—æ¨¡å‹ä¹‹é—´çš„é¢„æµ‹å·®å¼‚ç¨‹åº¦(Disagreement Rate çŸ©é˜µ)
    :param model_list: æ¨¡å‹å®ä¾‹åˆ—è¡¨ï¼ˆæ¯ä¸ªæ¨¡å‹åº”å…·æœ‰ .predict æ–¹æ³•ï¼‰
    :param val_x: éªŒè¯é›†ç‰¹å¾
    :return: N x N å¯¹ç§°çŸ©é˜µï¼Œå…ƒç´ [i][j] è¡¨ç¤ºæ¨¡å‹iä¸æ¨¡å‹jé¢„æµ‹ä¸ä¸€è‡´çš„æ ·æœ¬æ¯”ä¾‹ï¼ˆDisagreement Rateï¼‰
    å€¼è¶Šé«˜ â†’ æ¨¡å‹äº’è¡¥æ€§è¶Šå¼ºï¼›
    """
    n = len(best_fitted_model_instance_list)
    preds = [model.predict(val_x) for model in best_fitted_model_instance_list]

    matrix = np.zeros((n, n))

    for i in range(n):
        for j in range(i + 1, n):
            diff = preds[i] != preds[j]
            disagree_rate = np.mean(diff)
            matrix[i][j] = disagree_rate
            matrix[j][i] = disagree_rate  # å¯¹ç§°

    return matrix

# å¾—åˆ°æ¨¡å‹çš„ç”Ÿæˆé”™è¯¯æŒ‡ç¤ºçŸ©é˜µï¼ˆå‘é‡ï¼‰E
def _error_matrix(best_fitted_model_instance_list: List, val_x, val_y) -> np.ndarray:
    """
    ç”Ÿæˆé”™è¯¯æŒ‡ç¤ºçŸ©é˜µ Eï¼Œå½¢çŠ¶ä¸º (n_models, n_samples)ã€‚
    E[j, i] = 1 è¡¨ç¤ºç¬¬ j ä¸ªæ¨¡å‹åœ¨ç¬¬ i ä¸ªæ ·æœ¬ä¸Šé¢„æµ‹é”™è¯¯ï¼›å¦åˆ™ä¸º 0ã€‚
    é€‚ç”¨äºäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ï¼ˆåªçœ‹â€œæ˜¯å¦æ­£ç¡®â€ï¼‰ã€‚
    """
    y_true = np.asarray(val_y).ravel()
    n_models = len(best_fitted_model_instance_list)
    n_samples = y_true.shape[0]
    E = np.zeros((n_models, n_samples), dtype=np.uint8)

    for j, model in enumerate(best_fitted_model_instance_list):
        y_pred = np.asarray(model.predict(val_x)).ravel()
        if y_pred.shape[0] != n_samples:
            raise ValueError(f"Model {j} predict length {y_pred.shape[0]} != {n_samples}")
        E[j] = (y_pred != y_true).astype(np.uint8)
    return E

# åŠŸèƒ½ï¼šè·å–æ¨¡å‹åŒé”™è¯¯é¢„æµ‹å·®å¼‚çŸ©é˜µ
def get_model_double_error_differences(best_fitted_model_instance_list: List, val_x, val_y) -> np.ndarray:
    """
    ç»å…¸â€œdouble-faultâ€çŸ©é˜µï¼ˆä¸è¦åš 1âˆ’JointErr äº’è¡¥ï¼‰ï¼š
    è¿”å› NÃ—N å¯¹ç§°çŸ©é˜µ DFï¼Œå…¶ä¸­ DF[i, j] = åŒæ—¶å‡ºé”™çš„æ ·æœ¬æ¯”ä¾‹ = |{k: e_i(k)=1 ä¸” e_j(k)=1}| / |D_valid|ã€‚
    - å–å€¼èŒƒå›´ [0, 1]ï¼›è¶Šå°è¡¨ç¤ºä¸¤æ¨¡å‹â€œå…±åŒçŠ¯é”™â€è¶Šå°‘ï¼ˆå¤šæ ·æ€§è¶Šé«˜ï¼‰ã€‚
    - å¯¹è§’çº¿ DF[i, i] = æ¨¡å‹ i çš„å•ä½“é”™è¯¯ç‡ã€‚
    """
    E = _error_matrix(best_fitted_model_instance_list, val_x, val_y).astype(np.float64)
    n_samples = E.shape[1]
    # äº¤é›†è®¡æ•°ï¼šE @ E^Tï¼ˆæŠŠ True/1 å½“ä½œ 1 è®¡æ•°ï¼‰
    inter_counts = E @ E.T  # å½¢çŠ¶ (N, N)
    DF = inter_counts / float(n_samples)
    # æ•°å€¼ç¨³å®šï¼šé™åˆ¶åˆ° [0,1]
    np.clip(DF, 0.0, 1.0, out=DF)
    return DF

# åŠŸèƒ½ï¼šè·å–æ¨¡å‹ Jaccard-Fault çŸ©é˜µ äº¤é›†/å¹¶é›†
def get_model_jaccard_fault(best_fitted_model_instance_list: List, val_x, val_y) -> np.ndarray:
    """
    å¢å¼ºç‰ˆæ–¹æ¡ˆ 3ï¼šJaccard-Fault çŸ©é˜µã€‚
    å®šä¹‰æ¯ä¸ªæ¨¡å‹çš„â€œé”™è¯¯é›†åˆâ€ E_i = {k | æ¨¡å‹ i åœ¨æ ·æœ¬ k ä¸Šå‡ºé”™}ã€‚
    è¿”å› NÃ—N å¯¹ç§°çŸ©é˜µ Jï¼Œå…¶ä¸­ J[i, j] = |E_i âˆ© E_j| / |E_i âˆª E_j|ã€‚
      - è‹¥ä¸¤è€…ä»ä¸å‡ºé”™ä¸” |E_i âˆª E_j|=0ï¼Œåˆ™å®šä¹‰ J[i, j] = 0ï¼ˆé¿å… NaNï¼‰ã€‚
      - å–å€¼èŒƒå›´ [0, 1]ï¼›æ•°å€¼è¶Šå°ï¼Œè¯´æ˜ä¸¤æ¨¡å‹çš„é”™è¯¯é‡å è¶Šå°‘ï¼ˆäº’è¡¥æ€§è¶Šå¼ºï¼‰ã€‚
      - å¯¹è§’çº¿ J[i, i] = 1ï¼ˆå¦‚æœè¯¥æ¨¡å‹æœ‰è‡³å°‘ä¸€ä¸ªé”™è¯¯ï¼‰ï¼›è‹¥å®Œå…¨æ— é”™ï¼Œå®šä¹‰ä¸º 0ã€‚
    """
    E = _error_matrix(best_fitted_model_instance_list, val_x, val_y).astype(np.float64)
    # äº¤é›†è®¡æ•°
    inter_counts = E @ E.T  # (N, N)
    # æ¯ä¸ªæ¨¡å‹çš„é”™è¯¯æ•°
    err_counts = E.sum(axis=1)  # (N,)
    # å¹¶é›†è®¡æ•° |A âˆª B| = |A| + |B| âˆ’ |A âˆ© B|
    union_counts = err_counts[:, None] + err_counts[None, :] - inter_counts

    with np.errstate(divide='ignore', invalid='ignore'):
        J = inter_counts / union_counts
        # å¤„ç† union ä¸º 0 çš„æƒ…å†µï¼ˆä¸¤è€…éƒ½æ— é”™ï¼‰ï¼šå®šä¹‰ J=0
        J = np.where(union_counts == 0.0, 0.0, J)

    # å¯¹è§’çº¿å¤„ç†ï¼šè‹¥è¯¥æ¨¡å‹æœ‰é”™ï¼Œåˆ™ J[i,i]=1ï¼›å¦åˆ™ 0
    has_err = (err_counts > 0)
    np.fill_diagonal(J, 0.0)
    for i, flag in enumerate(has_err):
        if flag:
            J[i, i] = 1.0
        else:
            J[i, i] = 0.0

    # æ•°å€¼ç¨³å®šï¼šé™åˆ¶åˆ° [0,1]
    np.clip(J, 0.0, 1.0, out=J)
    return J

# åŠŸèƒ½ï¼šè·å–æ¨¡å‹è¡¨ç°å·®å¼‚æç¤ºè¯
def get_model_performance_differences_prompt(model_difference_matrix):
    """
    è·å–æ¨¡å‹è¡¨ç°å·®å¼‚æç¤ºè¯
    :param disagree_matrix: N x N å¯¹ç§°çŸ©é˜µï¼ˆnumpy array æˆ–åµŒå¥— listï¼‰
    :return: å­—ç¬¦ä¸²ï¼Œæè¿°æ€§æç¤ºè¯
    """
    n = len(model_difference_matrix)

    # æ ¼å¼åŒ–çŸ©é˜µä¸ºå­—ç¬¦ä¸²
    matrix_str = "[\n"
    for row in model_difference_matrix:
        formatted_row = ", ".join(f"{val:.4f}" for val in row)
        matrix_str += f" [{formatted_row}],\n"
    matrix_str = matrix_str.rstrip(",\n") + "\n]"

    prompt = (
    "The pairwise prediction differences on an independent validation set are shown in the matrix below. "
    "Each element [i][j] denotes the prediction disagreement rate between model i and model j on the validation set; "
    "a larger value indicates greater divergence in predictions and thus stronger complementarity.\n"
    "The model order in the matrix follows the list above (Model 1, Model 2, ...), and the matrix is 1-indexed to match the model numbering.\n"
    f"The disagreement matrix is as follows (unit: proportion, rounded to two decimal places):\n{matrix_str}\n"
    )

    return prompt

# åŒé”™è¯¯é¢„æµ‹å·®å¼‚çŸ©é˜µæç¤ºè¯
def get_model_double_error_prompt(model_double_error_matrix):
    """
    è·å–æ¨¡å‹åŒé”™è¯¯é¢„æµ‹å·®å¼‚çŸ©é˜µæç¤ºè¯
    :param model_double_error_matrix: N x N å¯¹ç§°çŸ©é˜µï¼ˆnumpy array æˆ–åµŒå¥— listï¼‰
    :return: å­—ç¬¦ä¸²ï¼Œæè¿°æ€§æç¤ºè¯
    """
    n = len(model_double_error_matrix)

    # æ ¼å¼åŒ–çŸ©é˜µä¸ºå­—ç¬¦ä¸²
    matrix_str = "[\n"
    for row in model_double_error_matrix:
        formatted_row = ", ".join(f"{val:.4f}" for val in row)
        matrix_str += f" [{formatted_row}],\n"
    matrix_str = matrix_str.rstrip(",\n") + "\n]"

    prompt = (
    "The double-fault prediction differences on an independent validation set are shown in the matrix below. "
    "Each element [i][j] denotes the proportion of samples on which model i and model j are simultaneously wrong on the validation set; "
    "a smaller value indicates fewer joint errors between the two models and thus greater diversity.\n"
    "The model order in the matrix follows the list above (Model 1, Model 2, ...), and the matrix is 1-indexed to match the model numbering.\n"
    f"The double-fault difference matrix is as follows (unit: proportion, rounded to two decimal places):\n{matrix_str}\n"
    )

    return prompt

# Jaccard-Fault çŸ©é˜µæç¤ºè¯
def get_model_intersection_union_prompt(model_jaccard_fault_matrix):
    """
    è·å–æ¨¡å‹ Jaccard-Fault çŸ©é˜µæç¤ºè¯
    :param model_jaccard_fault_matrix: N x N å¯¹ç§°çŸ©é˜µï¼ˆnumpy array æˆ–åµŒå¥— listï¼‰
    :return: å­—ç¬¦ä¸²ï¼Œæè¿°æ€§æç¤ºè¯
    """
    n = len(model_jaccard_fault_matrix)

    # æ ¼å¼åŒ–çŸ©é˜µä¸ºå­—ç¬¦ä¸²
    matrix_str = "[\n"
    for row in model_jaccard_fault_matrix:
        formatted_row = ", ".join(f"{val:.4f}" for val in row)
        matrix_str += f" [{formatted_row}],\n"
    matrix_str = matrix_str.rstrip(",\n") + "\n]"

    prompt = (
    "The Jaccard-Fault differences on an independent validation set are shown in the matrix below. "
    "Each element [i][j] denotes the Jaccard similarity between the error sets of model i and model j on the validation set; "
    "a smaller value indicates less overlap in their errors and thus stronger complementarity.\n"
    "The model order in the matrix follows the list above (Model 1, Model 2, ...), and the matrix is 1-indexed to match the model numbering.\n"
    f"The Jaccard-Fault matrix is as follows (unit: proportion, rounded to two decimal places):\n{matrix_str}\n"
    )

    return prompt

# æ•‘æ´-ç½®ä¿¡çŸ©é˜µ
def get_rescue_confidence_matrix(
    best_fitted_model_instance_list: List,
    val_x,
    val_y,
    task: str,
    beta: float = 5.0,
    high_err_quantile: float = 0.8,
    eps: float = 1e-12,
    label_smoothing: float = 1e-3,
) -> np.ndarray:
    """
    Rescueâ€“Confidence çŸ©é˜µ Rï¼ˆéå¯¹ç§°ï¼‰ï¼šR[i, j] è¡¨ç¤ºâ€œi åœ¨ j çŠ¯é”™æ—¶æŠŠå®ƒæ•‘å›çš„èƒ½åŠ›â€ã€‚
    ä½ éœ€æ˜¾å¼ä¼ å…¥ task âˆˆ {"classification", "regression"}ã€‚
    """
    models = best_fitted_model_instance_list
    N = len(models)
    val_y = np.asarray(val_y)

    if task == "classification":
        # ---------- ç»Ÿä¸€ç±»åˆ«ç©ºé—´ ----------
        classes = set(np.unique(val_y))
        for m in models:
            if hasattr(m, "classes_"):
                try:
                    classes.update(list(m.classes_))
                except Exception:
                    pass
        global_classes = np.array(sorted(list(classes)))
        C = len(global_classes)
        cls_index = {c: i for i, c in enumerate(global_classes)}

        # ç´¢å¼•åŒ–çœŸæ ‡ç­¾
        y_true_idx = np.vectorize(lambda c: cls_index[c])(val_y)
        M = len(val_y)

        preds_idx = []
        probas = []

        for m in models:
            y_pred = m.predict(val_x)
            y_pred_idx = np.vectorize(lambda c: cls_index[c])(y_pred)
            preds_idx.append(y_pred_idx)

            # ç›®æ ‡ï¼šæ„é€ å¯¹é½åˆ°å…¨å±€ç±»åˆ«çš„ P: (M, C)
            P = np.zeros((M, C), dtype=np.float64)

            if hasattr(m, "predict_proba"):
                p_local = m.predict_proba(val_x)
                p_local = np.asarray(p_local)

                # ç»Ÿä¸€æˆ (M, ?)
                if p_local.ndim == 1:
                    # ä¸€ç»´ â†’ è§†ä½œæ­£ç±»æ¦‚ç‡
                    if hasattr(m, "classes_") and len(getattr(m, "classes_")) == 2:
                        neg_cls, pos_cls = m.classes_[0], m.classes_[1]
                        pos_idx = cls_index[pos_cls]
                        neg_idx = cls_index[neg_cls]
                        p_pos = np.clip(p_local, 0.0, 1.0)
                        P[:, pos_idx] = p_pos
                        P[:, neg_idx] = 1.0 - p_pos
                    else:
                        # æ—  classes_ æˆ–å¤šç±»ä½†åªç»™äº†ä¸€åˆ—æ¦‚ç‡ï¼ˆä¸è§„èŒƒæƒ…å†µï¼‰â†’ é€€åŒ–å…œåº•
                        maxp = np.clip(p_local, 0.0, 1.0)
                        P[:] = 0.0
                        P[np.arange(M), y_pred_idx] = maxp
                        remain = 1.0 - maxp
                        if C > 1:
                            P += (remain / (C - 1))[:, None]
                            P[np.arange(M), y_pred_idx] -= remain / (C - 1)
                else:
                    # äºŒç»´ (M, C_m)
                    if hasattr(m, "classes_"):
                        # æŒ‰æ¨¡å‹è‡ªèº«ç±»é¡ºåºæ˜ å°„åˆ°å…¨å±€
                        for j_cls, cls in enumerate(m.classes_):
                            P[:, cls_index[cls]] = p_local[:, j_cls]
                    else:
                        # æ—  classes_ â†’ å°†æœ€å¤§åˆ—æ˜ å°„åˆ°é¢„æµ‹ç±»ï¼Œå…¶ä½™å‡åŒ€æ‘Š
                        maxp = p_local.max(axis=1)
                        P[np.arange(M), y_pred_idx] = maxp
                        remain = 1.0 - maxp
                        if C > 1:
                            P += (remain / (C - 1))[:, None]
                            P[np.arange(M), y_pred_idx] -= remain / (C - 1)
            else:
                # æ—  predict_probaï¼šç”¨ label smoothing ä¼ªæ¦‚ç‡
                eps_ls = float(label_smoothing)
                P[:] = eps_ls if C > 1 else 0.0
                if C > 1:
                    P[np.arange(M), y_pred_idx] = 1.0 - (C - 1) * eps_ls
                else:
                    P[:, 0] = 1.0

            # æ•°å€¼å‰ªè£ + å½’ä¸€
            P = np.clip(P, 1e-12, 1.0)
            P /= P.sum(axis=1, keepdims=True)
            probas.append(P)

        preds_idx = np.asarray(preds_idx)              # (N, M)
        probas = np.asarray(probas)                    # (N, M, C)

        # æ­£ç¡®/é”™è¯¯æ©ç 
        E = (preds_idx != y_true_idx[None, :]).astype(np.float64)  # j é”™
        Cmask = 1.0 - E                                            # i å¯¹

        # p_i(y|x)
        rows = np.arange(M)
        s_true = probas[:, rows, y_true_idx]           # (N, M)

        # margin_j(x) = p_j(Å·_j|x) - p_j(y|x)
        p_pred = probas[np.arange(N)[:, None], rows[None, :], preds_idx]  # (N, M)
        margin = p_pred - s_true

        # é”™æ ·æƒé‡
        z = np.clip(beta * margin, -50, 50)
        W = (1.0 / (1.0 + np.exp(-z))) * E             # (N, M)
        Z = W.sum(axis=1) + eps                        # (N,)

        # è®¡ç®— Rï¼ˆåˆ—æŒ‰ j å½’ä¸€ï¼‰
        CS = Cmask * s_true                            # (N, M)
        R = np.zeros((N, N), dtype=np.float64)
        for j in range(N):
            num = CS @ W[j]                            # (N,)
            R[:, j] = num / Z[j]

        np.fill_diagonal(R, 0.0)
        np.clip(R, 0.0, 1.0, out=R)
        return R

    elif task == "regression":
        # ---------- å›å½’è·¯å¾„ ----------
        preds = np.asarray([m.predict(val_x).astype(np.float64) for m in models])  # (N, M)
        y_true = val_y.astype(np.float64)
        resid = y_true[None, :] - preds                    # (N, M)
        abs_resid = np.abs(resid)

        R = np.zeros((N, N), dtype=np.float64)
        for j in range(N):
            # é«˜è¯¯å·®æ ·æœ¬é›†åˆï¼ˆj çš„éš¾é”™ï¼‰
            T = np.quantile(abs_resid[j], high_err_quantile)
            hard = (abs_resid[j] > T).astype(np.float64)   # (M,)
            w = abs_resid[j] * hard                        # (M,)
            Zj = w.sum() + eps

            # æ”¹å–„é‡ï¼š(|r_j|-|r_i|)_+ Â· |r_j| Â· 1[hard]
            diff = np.maximum(0.0, abs_resid[j][None, :] - abs_resid)     # (N, M)
            num = (diff * abs_resid[j][None, :] * hard[None, :]).sum(axis=1)  # (N,)
            R[:, j] = num / Zj

        np.fill_diagonal(R, 0.0)
        np.clip(R, 0.0, 1.0, out=R)
        return R

    else:
        raise ValueError("task å¿…é¡»ä¸º 'classification' æˆ– 'regression'")

# æ ¹æ®æ•‘æ´-ç½®ä¿¡çŸ©é˜µç”Ÿæˆ LLM æç¤ºè¯
def get_rescue_confidence_prompt(rescue_confidence_matrix):
    """
    è·å–æ•‘æ´-ç½®ä¿¡çŸ©é˜µçš„æç¤ºè¯
    :param rescue_confidence_matrix: N x N éå¯¹ç§°çŸ©é˜µï¼ˆnumpy array æˆ–åµŒå¥— listï¼‰
    :return: å­—ç¬¦ä¸²ï¼Œæè¿°æ€§æç¤ºè¯
    """
    n = len(rescue_confidence_matrix)

    # æ ¼å¼åŒ–çŸ©é˜µä¸ºå­—ç¬¦ä¸²
    matrix_str = "[\n"
    for row in rescue_confidence_matrix:
        formatted_row = ", ".join(f"{val:.4f}" for val in row)
        matrix_str += f" [{formatted_row}],\n"
    matrix_str = matrix_str.rstrip(",\n") + "\n]"

    prompt = (
    "The Rescue-Confidence matrix is shown below. "
    "Each element [i][j] indicates the ability of model i to rescue model j when it makes a mistake; "
    "a larger value means model i is more effective at correcting model j's errors.\n"
    "The model order in the matrix follows the list above (Model 1, Model 2, ...), and the matrix is 1-indexed to match the model numbering.\n"
    f"The Rescue-Confidence matrix is as follows (unit: proportion, rounded to two decimal places):\n{matrix_str}\n"
    )

    return prompt

def generate_llm_weight_prompt(
    base_prompt=None,
    dataset_description_prompt=None,
    model_code_prompt=None,
    model_performance_differences_prompt=None,
    model_double_error_prompt=None,
    model_Jaccard_Fault_prompt=None,
    model_rescue_confidence_prompt=None,
    single_shot_prompt_D=None,
    single_shot_prompt_J=None,
    single_shot_prompt_R=None,
):
    """
    Build the final LLM prompt for assigning ensemble weights. All instructions are in English.
    """
    # Initialize prompt parts with the fixed instructions
    prompt_parts = [
        """
You must output the following three parts:
(1) Output the weight for each model. A model's weight may be 0, and the sum of all weights must be 1.00.
(2) Briefly explain the reason for each model's weight.
(3) On the last line, output a weight list alone, in the format: Final model weight list: [0.13, 0.22, 0.00, 0.33, 0.32]
The last line will be parsed automatically by the program, so strictly follow this format.
"""
    ]

    # Append optional sections if provided
    if model_code_prompt is not None:
        prompt_parts.append(model_code_prompt.strip())

    if model_performance_differences_prompt is not None:
        prompt_parts.append(model_performance_differences_prompt.strip())
        prompt_parts.append(
            f"""
single-shot example:
{{
{single_shot_prompt_D.strip()}
}}
"""
        )

    if model_double_error_prompt is not None:
        prompt_parts.append(model_double_error_prompt.strip())

    if model_Jaccard_Fault_prompt is not None:
        prompt_parts.append(model_Jaccard_Fault_prompt.strip())
        prompt_parts.append(
            f"""
single-shot example:
{{
{single_shot_prompt_J.strip()}
}}
"""
        )

    if model_rescue_confidence_prompt is not None:
        prompt_parts.append(model_rescue_confidence_prompt.strip())
        prompt_parts.append(
            f"""
single-shot example:
{{
{single_shot_prompt_R.strip()}
}}
"""
        )

    # Final formatting constraint reminder
    prompt_parts.append(
        """
IMPORTANT: The last line of your output must exactly match the format of the last line in the single-shot example!
IMPORTANT:If the model weight is 0, you must also output it, and you must ensure that the dimension of the obtained weight vector is consistent with the number of models!
"""
    )
    return "\n\n".join(prompt_parts)

def extract_weight_list(llm_response: str):
    """
    ä» LLM è¿”å›çš„æ–‡æœ¬ä¸­æå–æƒé‡åˆ—è¡¨ï¼Œæ”¯æŒæ ¼å¼å¦‚ï¼š
    æœ€ç»ˆçš„æƒé‡åˆ—è¡¨ï¼š[0.00, 0.40, 0.00, 0.30, 0.30]
    æˆ– æ¨¡å‹æƒé‡åˆ—è¡¨ï¼š[0.20, 0.25, 0.00, 0.30, 0.25]
    ä»¥åŠ **Final model weight list:** [0.40, ...] ç­‰ Markdown åŠ ç²—å½¢å¼
    """
    print("\nğŸ” æ¨¡å‹æƒé‡æ¨ç†ç»“æœåŸå§‹è¾“å‡º:\n")
    print(llm_response)

    pattern = r"""
    (?ixs)
    (?:\*\*)?\s*
    (?:æœ€ç»ˆçš„?æƒé‡åˆ—è¡¨|æ¨¡å‹æƒé‡åˆ—è¡¨|æƒé‡åˆ—è¡¨|Final\s*model\s*weight\s*list)\s*
    (?:\*\*)?\s*
    [:ï¼š=]?
    \s*(?:\*\*)?\s*
    \[(.*?)\]
    """
    match = re.search(pattern, llm_response, re.IGNORECASE | re.DOTALL | re.VERBOSE)

    if match:
        raw_weights = match.group(1)
        try:
            weights = [float(w.strip()) for w in raw_weights.split(',')]
            print("\nâœ… æå–æˆåŠŸï¼Œæƒé‡å‘é‡ä¸ºï¼š", weights)
            return weights
        except Exception as e:
            print("âŒ æƒé‡æå–å¤±è´¥ï¼Œæ ¼å¼å¯èƒ½æœ‰è¯¯ï¼š", str(e))
            return []
    else:
        print("âŒ æœªåœ¨è¾“å‡ºä¸­æ‰¾åˆ°æƒé‡åˆ—è¡¨æ ‡è®°")
        return []

# ç”Ÿæˆä¸‹æ¸¸æ¨¡å‹ä»£ç 
def generate_model(model, messages):
    # åˆ›å»ºä¸€ä¸ª OpenAI çš„ API å®¢æˆ·ç«¯å®ä¾‹
    client = OpenAI(
        base_url='xxx',
        api_key='xxx',
    )
    # è¿™ä¸€æ®µæ˜¯è°ƒç”¨ OpenAI çš„ Chat Completion æ¥å£ï¼Œè®©æ¨¡å‹æ ¹æ® messages å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        stop=["```end"],
        temperature=0.7,
        max_tokens=700
    )
    # ä»æ¨¡å‹çš„è¿”å›ç»“æœä¸­æå–ç¬¬ä¸€æ¡ç”Ÿæˆçš„æ¶ˆæ¯å†…å®¹
    code = completion.choices[0].message.content
    code = code.replace("```python", "").replace("```", "").replace("<end>", "")
    return code

# è°ƒç”¨ openAI çš„ ChatGPT æ¥å£
def call_llm_chat_completion(model, messages):
    client = OpenAI(
        base_url='xxx',
        api_key='xxx',
    )

    try:
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            stop=["```end"],  # å¯é€‰ï¼šå¦‚éœ€æ§åˆ¶ç»ˆæ­¢
            temperature=0.4,
            max_tokens=700  # å®˜æ–¹å‚æ•°åæ˜¯ max_tokensï¼Œä¸æ˜¯ max_completion_tokens
        )
        return completion

    except Exception as e:
        print(f"âŒ è°ƒç”¨ OpenAI æ¥å£å¤±è´¥: {e}")
        return None
    

def get_model_prompt(ds_description=None):
    prompt = f"""
        Here is what I provide:

        Dataset description:
        {ds_description}

        Based on this information, you must:
        1. Generate a new Python classifier class named `myclassifier` each round.
        2. The model must differ from previous versions in **model type or structure**.
        3. The goal is to **maximize AUC (Area Under the ROC Curve)** on the given test data.

        The class must support the following methods:
            model = myclassifier()                        # Initialize the classifier
            model.fit(train_aug_x, train_aug_y)           # Train the model (both are pandas DataFrames)
            pred = model.predict(test_aug_x)              # Predict class labels (1D array or list)
            proba = model.predict_proba(test_aug_x)       # Predict probabilities of the positive class

        Important instructions (must follow **strictly**):
        - Only output the complete Python class named `myclassifier`. No explanation, no markdown, no extra output.
        - The class must be ready to use in Python (all necessary imports included, no undefined variables).
        - `train_aug_x`, `train_aug_y`, and `test_aug_x` are all pandas DataFrames.
        - `predict` must return a 1D array or list (same length as `test_aug_x`).
        - `predict_proba` must return the **positive class** probability:
              return self.model.predict_proba(test_x)[:, 1]
        - Each new version must differ from the previous ones (by model type or structure).
        - Use diverse models across rounds (e.g., LogisticRegression, RandomForest, GradientBoosting, etc.).
        - You may use libraries such as scikit-learn (>=1.4), XGBoost, CatBoost, etc.
        - **You must NOT use `LightGBM` under any circumstances. It is forbidden due to compatibility issues.**
        - Always prefer models that support probabilistic outputs suitable for AUC optimization.
        Output must be **only** the full Python class named `myclassifier`. No extra explanation or code.
        """
    return prompt


from typing import Tuple, Any
import numpy as np

def train_test_split_new(
    *arrays: Any,
    test_size: float | int = 0.25,
    train_size: float | int | None = None,
    random_state: int | None = None,
    shuffle: bool = True,
) -> Tuple[Any, ...]:
    """
    A minimal reimplementation of sklearn.model_selection.train_test_split.

    Parameters
    ----------
    arrays : one or more array-like, all with the same length (len = n_samples)
    test_size : float in (0,1) or int (number of test samples). Default 0.25
    train_size : float in (0,1) or int, optional. If None, it's n - n_test
    random_state : int seed for reproducibility
    shuffle : whether to shuffle before splitting

    Returns
    -------
    A tuple of length 2*len(arrays): (train_arr1, test_arr1, train_arr2, test_arr2, ...)
    """

    if len(arrays) == 0:
        raise ValueError("At least one array must be provided.")

    n = len(arrays[0])
    for a in arrays[1:]:
        if len(a) != n:
            raise ValueError("All input arrays must have the same length.")

    # Determine sizes
    if isinstance(test_size, float):
        if not (0.0 < test_size < 1.0):
            raise ValueError("When float, test_size must be in (0, 1).")
        n_test = int(np.ceil(n * test_size))
    elif isinstance(test_size, int):
        n_test = test_size
    else:
        raise TypeError("test_size must be float or int.")

    if train_size is None:
        n_train = n - n_test
    else:
        if isinstance(train_size, float):
            if not (0.0 < train_size < 1.0):
                raise ValueError("When float, train_size must be in (0, 1).")
            n_train = int(np.floor(n * train_size))
        elif isinstance(train_size, int):
            n_train = train_size
        else:
            raise TypeError("train_size must be float, int, or None.")

        if n_train + n_test > n:
            raise ValueError("train_size + test_size must be <= n_samples.")

    if n_train <= 0 or n_test <= 0:
        raise ValueError("Both train and test sizes must be >= 1.")

    # Build indices
    indices = np.arange(n)
    if shuffle:
        rng = np.random.default_rng(random_state)
        rng.shuffle(indices)

    # Here we take the first n_train as train, remaining as test
    train_idx = indices[:n_train]
    test_idx = indices[n_train:n_train + n_test]

    # Helper to index arrays while preserving type where possible
    def _index(a, idx):
        # numpy arrays, lists, tuples are supported; for others try __getitem__
        if isinstance(a, np.ndarray):
            return a[idx]
        try:
            return np.asarray(a)[idx]
        except Exception:
            # Fall back to Python-level indexing
            return [a[i] for i in idx]

    out = []
    for a in arrays:
        out.append(_index(a, train_idx))
        out.append(_index(a, test_idx))
    return tuple(out)


def to_pd(df_train, target_name):
    y = df_train[target_name].astype(int)
    x = df_train.drop(target_name, axis=1)
    return x, y


def get_classification_param_prompt_NOconstraint(
        best_code, best_auc, dataset_description,
        X_test, feature_columns, dataset_name=None, max_rows=10
    ):
        """
        ç”Ÿæˆé€‚ç”¨äº LLM åˆ†ç±»æ¨¡å‹å‚æ•°ä¼˜åŒ–çš„æç¤ºè¯
        """
        import pandas as pd
        if isinstance(X_test, pd.DataFrame):
            df_show = X_test.copy()
        else:
            df_show = pd.DataFrame(X_test, columns=feature_columns)

        table = df_show.head(max_rows).to_string(index=False)
        data_shape = df_show.shape if hasattr(df_show, 'shape') else (len(df_show), len(feature_columns))
        if dataset_name is None:
            dataset_name = "unknown"

        prompt = (
            f"Here is the best classification model code so far, with its current AUC score on the test set:\n\n"
            f"Current best AUC: {best_auc:.4f}\n\n"
            f"Model code:\n"
            f"```python\n{best_code}\n```\n"
            f"The downstream classification task is based on the following dataset.\n"
            f"Dataset name: {dataset_name}\n"
            f"Dataset description:\n{dataset_description}\n\n"
            # f"Feature names:\n{', '.join(feature_columns)}\n\n"
            f"Test set shape: {data_shape}\n"
            f"Here are the first {max_rows} rows of the test set:\n{table}\n\n"
            "Please ONLY optimize the hyperparameters\n"
            "in the given classification model code to further improve the AUC value.\n"
            "DO NOT change the algorithm type or model structure.\n"
            "Output only a new optimized Python code block.\n"
            "No explanation, only code!\n"
            "IMPORTANT CONTEXT:\n"
            "You are writing classification model code in Python using scikit-learn version 1.6.1.\n"
            "STRICT REQUIREMENT:\n"
            "ONLY use parameters that are supported by scikit-learn version 1.6.1.\n"
            "DO NOT use any parameters that are deprecated or only available in versions prior to 1.2.\n"
            "Refer ONLY to the scikit-learn 1.6.1 documentation for valid parameters and their default values."
        )

        return prompt