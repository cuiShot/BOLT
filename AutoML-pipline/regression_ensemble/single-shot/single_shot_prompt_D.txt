Model List:
model1:
- Validation set performance: RMSE=3.5885, MAE=2.3442, RMSLE=0.1581
model2:
- Validation set performance: RMSE=3.9637, MAE=2.5003, RMSLE=0.1623
model3:
- Validation set performance: RMSE=4.4063, MAE=2.9993, RMSLE=0.1894
model4:
- Validation set performance: RMSE=3.6421, MAE=2.4030, RMSLE=0.1525
model5:
- Validation set performance: RMSE=4.0396, MAE=2.5760, RMSLE=0.1658

Prediction-Difference matrix
The prediction-difference matrix is as follows (unit: normalized score in [0,1], rounded to four decimals):
[
 [0.0000, 0.0111, 0.0221, 0.0085, 0.0089],
 [0.0111, 0.0000, 0.0207, 0.0080, 0.0053],
 [0.0221, 0.0207, 0.0000, 0.0212, 0.0145],
 [0.0085, 0.0080, 0.0212, 0.0000, 0.0047],
 [0.0089, 0.0053, 0.0145, 0.0047, 0.0000]
]

Model Output (Recommended Optimal Weight Vector):

Model 1: Weight = 0.36
Model 2: Weight = 0.14
Model 3: Weight = 0.08
Model 4: Weight = 0.36
Model 5: Weight = 0.06  

Reasoning for Weight Assignment:
- Model 1 has the best RMSE (3.5885) and strong overall error profile; assign a core weight.
- Model 4 is close in RMSE (3.6421) and has the best RMSLE (0.1525), offering complementary error behavior; assign a core weight alongside Model 1.
- Model 3 performs worst by RMSE but shows high divergence with key models (e.g., [1,3] = 0.0221, [3,4] = 0.0212), indicating potential diversity gains; retain with a small weight.
- Model 2 is mid-tier by RMSE and moderately diverse vs Model 3 ([2,3] = 0.0207); keep a modest weight.
- Model 5 is weaker (RMSE 4.0396) and shows relatively lower divergence (e.g., [4,5] = 0.0047); assign a minimal weight.

Final model weight list: [0.36, 0.14, 0.08, 0.36, 0.06]
